{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce8fc3ef-d8e2-4aef-8efc-35cbe7da5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96dc2d38-3191-4693-b1c8-1769e1af4515",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set API key from environment\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"OpenAI API key not found. Make sure you have a .env file with OPENAI_API_KEY set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba5bf306-237f-49f8-82f1-75ed76e80f6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def encode_texts_openai(texts, cache_path=\"embeddings_cache.pkl\", model=\"text-embedding-3-large\"):\n",
    "    print(\"\\nEncoding texts using OpenAI model...\")\n",
    "    \n",
    "    # Load cache if exists\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            cache = pickle.load(f)\n",
    "    else:\n",
    "        cache = {}\n",
    "\n",
    "    embeddings = []\n",
    "    new_count = 0\n",
    "\n",
    "    for text in tqdm(texts, desc=\"Embedding\"):\n",
    "        key = hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "        if key in cache:\n",
    "            embeddings.append(cache[key])\n",
    "        else:\n",
    "            try:\n",
    "                response = openai.embeddings.create(input=text, model=model)\n",
    "                vec = response.data[0].embedding\n",
    "                cache[key] = vec\n",
    "                embeddings.append(vec)\n",
    "                new_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"OpenAI API failed on: {text[:50]}...\\n{e}\")\n",
    "                vec = [0.0] * 1536  # fallback, dim of text-embedding-3-small\n",
    "                embeddings.append(vec)\n",
    "\n",
    "    # Save updated cache\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "    print(f\"Embedding complete. {new_count} new vectors generated, {len(texts) - new_count} loaded from cache.\")\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "272fa567-4767-468c-bee0-54101350651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tb1_path, tb2_path):\n",
    "    # table1 = pd.read_csv(tb1_path)\n",
    "    # table2 = pd.read_csv(tb2_path)\n",
    "    table1 = pd.read_csv(tb1_path, encoding='cp1252')\n",
    "    table2 = pd.read_csv(tb2_path, encoding='cp1252')\n",
    "    # table1 = pd.read_csv(tb1_path, encoding='latin1')\n",
    "    # table2 = pd.read_csv(tb2_path, encoding='utf-8-sig')  # fixes BOM issue\n",
    "    \n",
    "\n",
    "    # Strip quotes and whitespace from column names\n",
    "    # table1.columns = table1.columns.str.strip().str.replace('\"', '')\n",
    "    # table2.columns = table2.columns.str.strip().str.replace('\"', '')\n",
    "    return table1, table2\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_table(df, columns_to_concat):\n",
    "    # Fill NaN for each selected column\n",
    "    for col in columns_to_concat:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in dataframe.\")\n",
    "        df[col] = df[col].fillna('')\n",
    "\n",
    "    # Apply preprocess and concatenate\n",
    "    processed_text = ''\n",
    "    for col in columns_to_concat:\n",
    "        processed_text += df[col].apply(preprocess) + ' '\n",
    "    \n",
    "    df['processed'] = processed_text.str.strip()\n",
    "    df['processed'] = df['processed'].replace('', 'unknown')\n",
    "    return df\n",
    "\n",
    "def preprocess_tables(table1, table2, table1_columns, table2_columns):\n",
    "    table1 = preprocess_table(table1, table1_columns)\n",
    "    table2 = preprocess_table(table2, table2_columns)\n",
    "    return table1, table2\n",
    "\n",
    "def encode_texts(model, texts, batch_size=64):\n",
    "    print(\"\\nEncoding texts into embeddings...\")\n",
    "    start_time = time.time()\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, batch_size=batch_size)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Embedding generation completed in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    if np.isnan(embeddings).any():\n",
    "        print(\"Warning: NaN values detected in embeddings - replacing with zeros\")\n",
    "        embeddings = np.nan_to_num(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1)\n",
    "    norms[norms == 0] = 1e-10\n",
    "    return embeddings / norms[:, np.newaxis]\n",
    "\n",
    "def build_faiss_index(embeddings, batch_size=1000):\n",
    "    print(\"\\nBuilding FAISS index...\")\n",
    "    start_time = time.time()\n",
    "    dimension = embeddings.shape[1]\n",
    "    if dimension == 0:\n",
    "        raise ValueError(\"Embedding dimension is 0 - check your input data\")\n",
    "    \n",
    "    try:\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        for i in tqdm(range(0, len(embeddings), batch_size), desc=\"Adding batches to FAISS\"):\n",
    "            batch = embeddings[i:i+batch_size]\n",
    "            if not np.isnan(batch).any():\n",
    "                index.add(batch)\n",
    "            else:\n",
    "                print(f\"Skipping batch {i} due to NaN values\")\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"FAISS index built in {elapsed:.2f} seconds.\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating FAISS index: {e}\")\n",
    "        print(\"Using brute-force matching instead\")\n",
    "        return None\n",
    "\n",
    "def find_matches(query_embeddings, table1, target_df, target_embeddings, index=None, top_k=30):\n",
    "    if index is None:\n",
    "        raise ValueError(\"FAISS index is not available. Matching cannot proceed.\")\n",
    "    print(\"\\nFinding matches...\")\n",
    "    matches = []\n",
    "    start_time = time.time()\n",
    "    for i, query_embedding in enumerate(tqdm(query_embeddings, desc=\"Matching queries\")):\n",
    "        if np.isnan(query_embedding).any():\n",
    "            print(f\"Skipping query {i} due to NaN values\")\n",
    "            continue\n",
    "\n",
    "        if index:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "            distances, indices = index.search(query_embedding, top_k)\n",
    "            distances = distances[0]\n",
    "            indices = indices[0]\n",
    "        # else:\n",
    "        #     similarities = np.dot(target_embeddings, query_embedding)\n",
    "        #     indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        #     distances = similarities[indices]\n",
    "\n",
    "        for score, idx in zip(distances, indices):\n",
    "            matches.append({\n",
    "                'left_id': table1.iloc[i]['id'],\n",
    "                'right_id': target_df.iloc[idx]['id'],\n",
    "                'similarity_score': score,\n",
    "                'table1_text': table1.iloc[i]['processed'],\n",
    "                'table2_text': target_df.iloc[idx]['processed']\n",
    "            })\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Matching completed in {elapsed:.2f} seconds.\")\n",
    "    return matches\n",
    "\n",
    "def save_results(matches_df, index=None, matches_file=\"entity_matches.csv\", index_file=\"entity_matching_index.faiss\"):\n",
    "    matches_df.to_csv(matches_file, index=False)\n",
    "    print(f\"\\nSaved matches to {matches_file}.\")\n",
    "    if index:\n",
    "        faiss.write_index(index, index_file)\n",
    "        print(f\"Saved FAISS index to {index_file}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183232ed-b94f-47dc-96aa-5c700ad5dbe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecce574b32b04a5d97d23f4ee1a59c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation completed in 1.21 seconds.\n",
      "\n",
      "Encoding texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99439ae448a14b9ca3ae2fe9cf05c33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation completed in 0.67 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████████████| 2/2 [00:00<00:00, 2114.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████████| 1081/1081 [00:00<00:00, 10470.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 0.10 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████████████| 2/2 [00:00<00:00, 2020.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|██████████████████████████████████████| 1081/1081 [00:00<00:00, 3249.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 0.33 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████████████| 2/2 [00:00<00:00, 2254.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|██████████████████████████████████████| 1081/1081 [00:00<00:00, 1877.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 0.58 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████████████| 2/2 [00:00<00:00, 2944.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|██████████████████████████████████████| 1081/1081 [00:00<00:00, 1290.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 0.84 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████████████| 2/2 [00:00<00:00, 3663.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|███████████████████████████████████████| 1081/1081 [00:01<00:00, 984.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 1.10 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████████████| 2/2 [00:00<00:00, 2816.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|███████████████████████████████████████| 1081/1081 [00:01<00:00, 793.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 1.36 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████████████| 2/2 [00:00<00:00, 2989.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|███████████████████████████████████████| 1081/1081 [00:01<00:00, 665.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 1.62 seconds.\n",
      "\n",
      "Evaluation Results:\n",
      " k   recall  precision  f1_score  embedding_time  build_time  search_time  total_pairs  true_positives     cssr\n",
      " 1 0.705561   0.716004  0.710744         1.88559    0.003065     0.104206         1081             774 0.000916\n",
      " 5 0.919781   0.186679  0.310366         1.88559    0.002463     0.334103         5405            1009 0.004579\n",
      "10 0.960802   0.097502  0.177039         1.88559    0.002046     0.576956        10810            1054 0.009158\n",
      "15 0.974476   0.065927  0.123498         1.88559    0.001831     0.839056        16215            1069 0.013736\n",
      "20 0.980857   0.049769  0.094731         1.88559    0.001469     1.099950        21620            1076 0.018315\n",
      "25 0.987238   0.040074  0.077022         1.88559    0.001909     1.363760        27025            1083 0.022894\n",
      "30 0.990884   0.033518  0.064843         1.88559    0.001621     1.625386        32430            1087 0.027473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# eval for apt-buy\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    tb1_path = os.path.join(\"Abt-Buy\", \"Abt.csv\")\n",
    "    tb2_path = os.path.join(\"Abt-Buy\", \"Buy.csv\")\n",
    "    ground_truth_path = os.path.join(\"Abt-Buy\", \"abt_buy_perfectMapping.csv\")\n",
    "\n",
    "    # Load and preprocess data\n",
    "    table1, table2 = load_data(tb1_path, tb2_path)\n",
    "    table1_columns = [\"name\"]\n",
    "    table2_columns = [\"name\"]\n",
    "    table1, table2 = preprocess_tables(table1, table2, table1_columns, table2_columns)\n",
    "\n",
    "    # Load ground truth\n",
    "    ground_truth = pd.read_csv(ground_truth_path)\n",
    "    ground_truth_set = set(zip(ground_truth['idAbt'], ground_truth['idBuy']))\n",
    "\n",
    "    # Load model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Generate and normalize embeddings (with timing)\n",
    "    start_time = time.time()\n",
    "    embeddings1 = encode_texts(model, table1['processed'].tolist())\n",
    "    embeddings2 = encode_texts(model, table2['processed'].tolist())\n",
    "    embeddings1 = normalize_embeddings(embeddings1)\n",
    "    embeddings2 = normalize_embeddings(embeddings2)\n",
    "    embedding_time = time.time() - start_time\n",
    "\n",
    "    # Test different k values\n",
    "    k_values = [1, 5, 10, 15, 20, 25, 30]\n",
    "    results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        # Build FAISS index (with timing)\n",
    "        start_time = time.time()\n",
    "        index = build_faiss_index(embeddings2)\n",
    "        build_time = time.time() - start_time\n",
    "\n",
    "        # Find matches (with timing)\n",
    "        start_time = time.time()\n",
    "        matches = find_matches(embeddings1, table1, table2, embeddings2, index=index, top_k=k)\n",
    "        search_time = time.time() - start_time\n",
    "\n",
    "        matches_df = pd.DataFrame(matches)\n",
    "        predicted_set = set(zip(matches_df['left_id'], matches_df['right_id']))\n",
    "\n",
    "        # Compute metrics\n",
    "        true_positives = predicted_set & ground_truth_set\n",
    "        recall = len(true_positives) / len(ground_truth_set)\n",
    "        precision = len(true_positives) / len(predicted_set) if len(predicted_set) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'recall': recall,\n",
    "            'precision': precision,\n",
    "            'f1_score': f1_score,\n",
    "            'embedding_time': embedding_time,\n",
    "            'build_time': build_time,\n",
    "            'search_time': search_time,\n",
    "            'total_pairs': len(predicted_set),\n",
    "            'true_positives': len(true_positives)\n",
    "        })\n",
    "\n",
    "    # Print results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    # Optionally save results\n",
    "    results_df.to_csv(\"blocking_evaluation_results_apt-buy.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc783fe3-745a-4e4a-ac23-327cd9998da5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e321533440844aaf83b2a2bc340917cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation completed in 6.49 seconds.\n",
      "\n",
      "Encoding texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea8dcf5c47347a4979b944685f298b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation completed in 2.76 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|███████████████████████████████| 6/6 [00:00<00:00, 1455.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.01 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|████████████████████████████████| 4259/4259 [00:00<00:00, 4382.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 0.97 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|███████████████████████████████| 6/6 [00:00<00:00, 1676.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.01 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|████████████████████████████████| 4259/4259 [00:02<00:00, 1758.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 2.42 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|███████████████████████████████| 6/6 [00:00<00:00, 2934.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|████████████████████████████████| 4259/4259 [00:03<00:00, 1328.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 3.21 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|███████████████████████████████| 6/6 [00:00<00:00, 2508.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|████████████████████████████████| 4259/4259 [00:04<00:00, 1004.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 4.24 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|███████████████████████████████| 6/6 [00:00<00:00, 2383.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 4259/4259 [00:05<00:00, 807.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 5.28 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|███████████████████████████████| 6/6 [00:00<00:00, 2110.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 4259/4259 [00:06<00:00, 656.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 6.49 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|███████████████████████████████| 6/6 [00:00<00:00, 2148.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.00 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 4259/4259 [00:07<00:00, 568.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 7.49 seconds.\n",
      "\n",
      "Sample matches (k=30):\n",
      "       left_id  right_id  similarity_score  \\\n",
      "59610     1988      3563          0.983699   \n",
      "71850     2396      2461          0.970228   \n",
      "30780     1027      2542          0.967111   \n",
      "60390     2014      2461          0.963338   \n",
      "23880      797      2459          0.959631   \n",
      "\n",
      "                                             table1_text  \\\n",
      "59610  other 3m privacy filter for widescreen laptop ...   \n",
      "71850  other case logic pls-13 neoprene 133-inch neop...   \n",
      "30780  other case logic laps-114 14-inch laptop sleev...   \n",
      "60390  other case logic pls-14 14-inch neoprene lapto...   \n",
      "23880  other case logic laps-117 17 - 173 -inch lapto...   \n",
      "\n",
      "                                             table2_text  \n",
      "59610  other 3m privacy filter for widescreen laptop ...  \n",
      "71850  other case logic neoprene laptop sleeve black ...  \n",
      "30780    other case logic laptop sleeve black laps-114bl  \n",
      "60390  other case logic neoprene laptop sleeve black ...  \n",
      "23880      other case logic laptop sleeve black laps-117  \n",
      "\n",
      "Total ground-truth matches: 58\n",
      "\n",
      "Evaluation Results:\n",
      " k  recall  precision  f1_score  reduction_ratio  embedding_time  build_time  search_time  total_pairs  true_positives\n",
      " 1  0.1379     0.0019    0.0037           0.9656          9.2676      0.0085       0.9733         4259               8\n",
      " 5  0.4483     0.0012    0.0024           0.8281          9.2676      0.0053       2.4243        21295              26\n",
      "10  0.5517     0.0008    0.0015           0.6562          9.2676      0.0034       3.2083        42590              32\n",
      "15  0.6034     0.0005    0.0011           0.4842          9.2676      0.0036       4.2416        63885              35\n",
      "20  0.7069     0.0005    0.0010           0.3123          9.2676      0.0036       5.2798        85180              41\n",
      "25  0.7414     0.0004    0.0008           0.1404          9.2676      0.0042       6.4945       106475              43\n",
      "30  0.7759     0.0004    0.0007          -0.0315          9.2676      0.0040       7.4955       127770              45\n",
      "\n",
      "Results saved to amazon_bestbuy_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# eval for amazon-best buy\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    tb1_path = os.path.join(\"files\", \"amazon.csv\")\n",
    "    tb2_path = os.path.join(\"files\", \"best_buy.csv\")\n",
    "    ground_truth_path = os.path.join(\"files\", \"labeled_data.csv\")\n",
    "\n",
    "    # Load and preprocess data\n",
    "    table1, table2 = load_data(tb1_path, tb2_path)\n",
    "    table1_columns = [\"Brand\", \"Name\"]\n",
    "    table2_columns = [\"Brand\", \"Name\"]\n",
    "    table1, table2 = preprocess_tables(table1, table2, table1_columns, table2_columns)\n",
    "\n",
    "    # Load ground truth (with special handling for header row)\n",
    "    ground_truth_df = pd.read_csv(ground_truth_path, skiprows=5)\n",
    "    ground_truth_matches = set(zip(\n",
    "        ground_truth_df.loc[ground_truth_df['gold'] == 1, 'ltable.ID'],\n",
    "        ground_truth_df.loc[ground_truth_df['gold'] == 1, 'rtable.ID']\n",
    "    ))\n",
    "    n_ltable = ground_truth_df['ltable.ID'].nunique()\n",
    "    n_rtable = ground_truth_df['rtable.ID'].nunique()\n",
    "    total_possible_pairs = n_ltable * n_rtable\n",
    "\n",
    "    # Load model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Time embedding generation\n",
    "    start_time = time.time()\n",
    "    embeddings1 = encode_texts(model, table1['processed'].tolist())\n",
    "    embeddings2 = encode_texts(model, table2['processed'].tolist())\n",
    "    embeddings1 = normalize_embeddings(embeddings1)\n",
    "    embeddings2 = normalize_embeddings(embeddings2)\n",
    "    embedding_time = time.time() - start_time\n",
    "\n",
    "    # Test different k values\n",
    "    k_values = [1, 5, 10, 15, 20, 25, 30]\n",
    "    results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        # Time index building\n",
    "        start_time = time.time()\n",
    "        index = build_faiss_index(embeddings2)\n",
    "        build_time = time.time() - start_time\n",
    "\n",
    "        # Time matching\n",
    "        start_time = time.time()\n",
    "        matches = find_matches(embeddings1, table1, table2, embeddings2, index=index, top_k=k)\n",
    "        search_time = time.time() - start_time\n",
    "\n",
    "        matches_df = pd.DataFrame(matches)\n",
    "        candidate_set = set(zip(matches_df['left_id'], matches_df['right_id']))\n",
    "\n",
    "        # Compute metrics\n",
    "        true_positives = ground_truth_matches.intersection(candidate_set)\n",
    "        recall = len(true_positives) / len(ground_truth_matches) if len(ground_truth_matches) > 0 else 0.0\n",
    "        precision = len(true_positives) / len(candidate_set) if len(candidate_set) > 0 else 0.0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        reduction_ratio = 1 - (len(candidate_set) / total_possible_pairs)\n",
    "\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'recall': recall,\n",
    "            'precision': precision,\n",
    "            'f1_score': f1_score,\n",
    "            'reduction_ratio': reduction_ratio,\n",
    "            'embedding_time': embedding_time,\n",
    "            'build_time': build_time,\n",
    "            'search_time': search_time,\n",
    "            'total_pairs': len(candidate_set),\n",
    "            'true_positives': len(true_positives)\n",
    "        })\n",
    "\n",
    "        # Show sample matches for k=30 (original value)\n",
    "        if k == 30:\n",
    "            print(\"\\nSample matches (k=30):\")\n",
    "            print(matches_df.sort_values(by='similarity_score', ascending=False).head())\n",
    "\n",
    "    # Print results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"\\nTotal ground-truth matches: {len(ground_truth_matches)}\")\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(results_df.to_string(index=False, float_format=\"{:0.4f}\".format))\n",
    "\n",
    "    # Save results\n",
    "    results_df.to_csv(\"amazon_bestbuy_evaluation_results.csv\", index=False)\n",
    "    print(\"\\nResults saved to amazon_bestbuy_evaluation_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33798b04-97f1-42d5-be2c-e499c11032a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc5e1bf61cc4ce88938c46f44d43972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation completed in 1.96 seconds.\n",
      "\n",
      "Encoding texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcce3f799034ed29dd913dc8c1513e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation completed in 30.40 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|██████████████████████████████| 65/65 [00:00<00:00, 957.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.07 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2616/2616 [00:06<00:00, 408.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 6.40 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|██████████████████████████████| 65/65 [00:00<00:00, 463.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.14 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2616/2616 [00:06<00:00, 391.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 6.69 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 65/65 [00:00<00:00, 1623.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.04 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2616/2616 [00:07<00:00, 352.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 7.43 seconds.\n",
      "\n",
      "Sample matches (k=10):\n",
      "                            left_id  \\\n",
      "7580       journals/sigmod/Sidell96   \n",
      "8640         journals/sigmod/Fong95   \n",
      "25470     journals/vldb/PeckhamMD95   \n",
      "10140     journals/sigmod/YeungHL94   \n",
      "1570   journals/sigmod/EisenbergM01   \n",
      "\n",
      "                                                right_id  similarity_score  \\\n",
      "7580   url:http://portal.acm.org/ft_gateway.cfm%3Fid%...               1.0   \n",
      "8640                                        zw3-t-veNvAJ               1.0   \n",
      "25470                                       zOfVTNgEwz0J               1.0   \n",
      "10140                                       yQv94p3tE1IJ               1.0   \n",
      "1570                                        GKUflKzgAVEJ               1.0   \n",
      "\n",
      "                                             table1_text  \\\n",
      "7580   the mariposa distributed database management s...   \n",
      "8640   mapping extended entity relationship model to ...   \n",
      "25470  data model for extensible support of explicit ...   \n",
      "10140  performance evaluation of a new distributed de...   \n",
      "1570   sqlxml and the sqlx informal group of companie...   \n",
      "\n",
      "                                             table2_text  \n",
      "7580   the mariposa distributed database management s...  \n",
      "8640   mapping extended entity relationship model to ...  \n",
      "25470  data model for extensible support of explicit ...  \n",
      "10140  performance evaluation of a new distributed de...  \n",
      "1570   sqlxml and the sqlx informal group of companie...  \n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 65/65 [00:00<00:00, 1017.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.07 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2616/2616 [00:08<00:00, 320.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 8.15 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 65/65 [00:00<00:00, 1509.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.04 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2616/2616 [00:08<00:00, 293.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 8.92 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 65/65 [00:00<00:00, 1306.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.05 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2616/2616 [00:09<00:00, 273.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 9.55 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 65/65 [00:00<00:00, 1196.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.06 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2616/2616 [00:10<00:00, 253.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 10.33 seconds.\n",
      "\n",
      "Evaluation Results:\n",
      " k  recall  precision  f1_score  embedding_time  build_time  search_time  total_pairs  true_positives\n",
      " 1  0.4410     0.9014    0.5922         32.4345      0.0710       6.4050         2616            2358\n",
      " 5  0.8837     0.3612    0.5128         32.4345      0.1436       6.6885        13080            4725\n",
      "10  0.9609     0.1964    0.3261         32.4345      0.0438       7.4321        26160            5138\n",
      "15  0.9792     0.1334    0.2349         32.4345      0.0667       8.1529        39240            5236\n",
      "20  0.9856     0.1007    0.1828         32.4345      0.0478       8.9266        52320            5270\n",
      "25  0.9878     0.0808    0.1493         32.4345      0.0535       9.5574        65400            5282\n",
      "30  0.9892     0.0674    0.1262         32.4345      0.0579      10.3348        78480            5289\n",
      "\n",
      "Results saved to dblp_scholar_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# eval for DBLP-Scholar\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    tb1_path = os.path.join(\"DBLP-Scholar\", \"DBLP1.csv\")\n",
    "    tb2_path = os.path.join(\"DBLP-Scholar\", \"Scholar.csv\")\n",
    "    ground_truth_path = os.path.join(\"DBLP-Scholar\", \"DBLP-Scholar_perfectMapping.csv\")\n",
    "\n",
    "    # Load and preprocess data\n",
    "    table1, table2 = load_data(tb1_path, tb2_path)\n",
    "    table1_columns = [\"title\", \"authors\"]\n",
    "    table2_columns = [\"title\", \"authors\"]\n",
    "    table1, table2 = preprocess_tables(table1, table2, table1_columns, table2_columns)\n",
    "\n",
    "    # Load ground truth\n",
    "    ground_truth = pd.read_csv(ground_truth_path)\n",
    "    ground_truth_set = set(zip(ground_truth['idDBLP'], ground_truth['idScholar']))\n",
    "\n",
    "    # Load model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Time embedding generation\n",
    "    start_time = time.time()\n",
    "    embeddings1 = encode_texts(model, table1['processed'].tolist())\n",
    "    embeddings2 = encode_texts(model, table2['processed'].tolist())\n",
    "    embeddings1 = normalize_embeddings(embeddings1)\n",
    "    embeddings2 = normalize_embeddings(embeddings2)\n",
    "    embedding_time = time.time() - start_time\n",
    "\n",
    "    # Test different k values\n",
    "    k_values = [1, 5, 10, 15, 20, 25, 30]\n",
    "    results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        # Time index building\n",
    "        start_time = time.time()\n",
    "        index = build_faiss_index(embeddings2)\n",
    "        build_time = time.time() - start_time\n",
    "\n",
    "        # Time matching\n",
    "        start_time = time.time()\n",
    "        matches = find_matches(embeddings1, table1, table2, embeddings2, index=index, top_k=k)\n",
    "        search_time = time.time() - start_time\n",
    "\n",
    "        matches_df = pd.DataFrame(matches)\n",
    "        predicted_set = set(zip(matches_df['left_id'], matches_df['right_id']))\n",
    "\n",
    "        # Compute metrics\n",
    "        true_positives = predicted_set & ground_truth_set\n",
    "        recall = len(true_positives) / len(ground_truth_set) if len(ground_truth_set) > 0 else 0\n",
    "        precision = len(true_positives) / len(predicted_set) if len(predicted_set) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'recall': recall,\n",
    "            'precision': precision,\n",
    "            'f1_score': f1_score,\n",
    "            'embedding_time': embedding_time,\n",
    "            'build_time': build_time,\n",
    "            'search_time': search_time,\n",
    "            'total_pairs': len(predicted_set),\n",
    "            'true_positives': len(true_positives)\n",
    "        })\n",
    "\n",
    "        # Show sample matches for k=10\n",
    "        if k == 10:\n",
    "            print(\"\\nSample matches (k=10):\")\n",
    "            print(matches_df.sort_values(by='similarity_score', ascending=False).head())\n",
    "\n",
    "    # Print results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(results_df.to_string(index=False, float_format=\"{:0.4f}\".format))\n",
    "\n",
    "    # Save results\n",
    "    results_df.to_csv(\"dblp_scholar_evaluation_results_DBLP-Scholar.csv\", index=False)\n",
    "    print(\"\\nResults saved to dblp_scholar_evaluation_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c3a3b6e-6b03-43c0-ad8d-5f29f1f61a13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3682b6ff772c4c14850be6c45dcf8dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation completed in 3.78 seconds.\n",
      "\n",
      "Encoding texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1d2a81a24c4e408236153e836a67bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation completed in 23.26 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 23/23 [00:00<00:00, 1049.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.02 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2554/2554 [00:02<00:00, 862.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 2.96 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 23/23 [00:00<00:00, 1387.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.02 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2554/2554 [00:03<00:00, 810.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 3.15 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 23/23 [00:00<00:00, 2103.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.01 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2554/2554 [00:04<00:00, 621.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 4.11 seconds.\n",
      "\n",
      "Sample matches (k=10):\n",
      "       left_id  right_id  similarity_score  \\\n",
      "15780     1579      3324          0.990323   \n",
      "12820     1283      1241          0.985462   \n",
      "9610       962     14318          0.983938   \n",
      "7220       723     12996          0.970382   \n",
      "10730     1074      2523          0.969655   \n",
      "\n",
      "                                             table1_text  \\\n",
      "15780              v7 mice v7 3-button optical mouse usb   \n",
      "12820  d-link networking d-link systems powerline av ...   \n",
      "9610   amped wireless networking amped wireless high ...   \n",
      "7220   corsair memory corsair memory vs2gbkit400c3 2 ...   \n",
      "10730  corsair memory corsair hx3x12g1333c9 12 gb xms...   \n",
      "\n",
      "                                             table2_text  \n",
      "15780             v7 mice  v7 3 button usb optical mouse  \n",
      "12820  d-link powerline network adapters  d-link syst...  \n",
      "9610   amped wireless routers  amped wireless high po...  \n",
      "7220   corsair memory  corsair memory vs2gbkit400c3 2...  \n",
      "10730  corsair memory  corsair hx3x12g1333c9 12 gb xm...  \n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 23/23 [00:00<00:00, 2324.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.01 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2554/2554 [00:04<00:00, 515.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 4.96 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 23/23 [00:00<00:00, 2334.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.01 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2554/2554 [00:05<00:00, 428.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 5.96 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 23/23 [00:00<00:00, 2228.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.01 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2554/2554 [00:06<00:00, 370.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 6.90 seconds.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|█████████████████████████████| 23/23 [00:00<00:00, 2264.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.01 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|█████████████████████████████████| 2554/2554 [00:07<00:00, 324.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 7.86 seconds.\n",
      "\n",
      "Total ground-truth matches: 1154\n",
      "Total possible pairs: 56376996\n",
      "\n",
      "Evaluation Results:\n",
      " k  recall  precision  f1_score  reduction_ratio  pairs_quality  embedding_time  build_time  search_time  total_pairs  true_positives  false_positives  false_negatives\n",
      " 1  0.4913     0.2220    0.3058           1.0000         0.2220         27.0644      0.0235       2.9622         2554             567             1987              587\n",
      " 5  0.8007     0.0724    0.1327           0.9998         0.0724         27.0644      0.0192       3.1529        12770             924            11846              230\n",
      "10  0.8951     0.0404    0.0774           0.9995         0.0404         27.0644      0.0128       4.1099        25540            1033            24507              121\n",
      "15  0.9272     0.0279    0.0542           0.9993         0.0279         27.0644      0.0120       4.9608        38310            1070            37240               84\n",
      "20  0.9437     0.0213    0.0417           0.9991         0.0213         27.0644      0.0117       5.9642        51080            1089            49991               65\n",
      "25  0.9506     0.0172    0.0338           0.9989         0.0172         27.0644      0.0124       6.9070        63850            1097            62753               57\n",
      "30  0.9567     0.0144    0.0284           0.9986         0.0144         27.0644      0.0124       7.8677        76620            1104            75516               50\n",
      "\n",
      "Results saved to walmart_amazon_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# eval for walmart_amazon\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    tb1_path = os.path.join(\"walmart_amazon\", \"tableA.csv\")\n",
    "    tb2_path = os.path.join(\"walmart_amazon\", \"tableB.csv\")\n",
    "    ground_truth_path = os.path.join(\"walmart_amazon\", \"matches.csv\")\n",
    "\n",
    "    # Load and preprocess data\n",
    "    table1, table2 = load_data(tb1_path, tb2_path)\n",
    "    table1_columns = [\"brand\", \"groupname\", \"title\", \"shortdescr\"]\n",
    "    table2_columns = [\"brand\", \"category1\", \"category2\", \"title\", \"proddescrshort\"]\n",
    "    table1, table2 = preprocess_tables(table1, table2, table1_columns, table2_columns)\n",
    "    # print(table1.columns)\n",
    "    # print(table2.columns)\n",
    "    # Load ground truth\n",
    "    ground_truth = pd.read_csv(ground_truth_path)\n",
    "    ground_truth_set = set(zip(ground_truth['id1'], ground_truth['id2']))\n",
    "    total_entities_left = table1['custom_id'].nunique()\n",
    "    total_entities_right = table2['custom_id'].nunique()\n",
    "    total_possible_pairs = total_entities_left * total_entities_right\n",
    "\n",
    "    # Load model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Time embedding generation\n",
    "    start_time = time.time()\n",
    "    embeddings1 = encode_texts(model, table1['processed'].tolist())\n",
    "    embeddings2 = encode_texts(model, table2['processed'].tolist())\n",
    "    embeddings1 = normalize_embeddings(embeddings1)\n",
    "    embeddings2 = normalize_embeddings(embeddings2)\n",
    "    embedding_time = time.time() - start_time\n",
    "\n",
    "    # Test different k values\n",
    "    k_values = [1, 5, 10, 15, 20, 25, 30]\n",
    "    results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        # Time index building\n",
    "        start_time = time.time()\n",
    "        index = build_faiss_index(embeddings2)\n",
    "        build_time = time.time() - start_time\n",
    "\n",
    "        # Time matching\n",
    "        start_time = time.time()\n",
    "        matches = find_matches(embeddings1, table1, table2, embeddings2, index=index, top_k=k)\n",
    "        search_time = time.time() - start_time\n",
    "\n",
    "        matches_df = pd.DataFrame(matches)\n",
    "        predicted_set = set(zip(matches_df['left_id'], matches_df['right_id']))\n",
    "\n",
    "        # Compute metrics\n",
    "        true_positives = predicted_set & ground_truth_set\n",
    "        recall = len(true_positives) / len(ground_truth_set)\n",
    "        precision = len(true_positives) / len(predicted_set) if len(predicted_set) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        reduction_ratio = 1 - (len(predicted_set) / total_possible_pairs)\n",
    "        pairs_quality = precision  # Same as precision\n",
    "\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'recall': recall,\n",
    "            'precision': precision,\n",
    "            'f1_score': f1_score,\n",
    "            'reduction_ratio': reduction_ratio,\n",
    "            'pairs_quality': pairs_quality,\n",
    "            'embedding_time': embedding_time,\n",
    "            'build_time': build_time,\n",
    "            'search_time': search_time,\n",
    "            'total_pairs': len(predicted_set),\n",
    "            'true_positives': len(true_positives),\n",
    "            'false_positives': len(predicted_set - ground_truth_set),\n",
    "            'false_negatives': len(ground_truth_set - predicted_set)\n",
    "        })\n",
    "\n",
    "        # Show sample matches for k=10 (original value)\n",
    "        if k == 10:\n",
    "            print(\"\\nSample matches (k=10):\")\n",
    "            print(matches_df.sort_values(by='similarity_score', ascending=False).head())\n",
    "\n",
    "    # Print results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"\\nTotal ground-truth matches: {len(ground_truth_set)}\")\n",
    "    print(f\"Total possible pairs: {total_possible_pairs}\")\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(results_df.to_string(index=False, float_format=\"{:0.4f}\".format))\n",
    "\n",
    "    # Save results\n",
    "    results_df.to_csv(\"walmart_amazon_evaluation_results.csv\", index=False)\n",
    "    print(\"\\nResults saved to walmart_amazon_evaluation_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4680b4a-22ef-4794-908a-444e70f75e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
