{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8fc3ef-d8e2-4aef-8efc-35cbe7da5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96dc2d38-3191-4693-b1c8-1769e1af4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set API key from environment\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"OpenAI API key not found. Make sure you have a .env file with OPENAI_API_KEY set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba5bf306-237f-49f8-82f1-75ed76e80f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts_openai(texts, cache_path=\"embeddings_cache.pkl\", model=\"text-embedding-3-large\"):\n",
    "    print(\"\\nEncoding texts using OpenAI model...\")\n",
    "    \n",
    "    # Load cache if exists\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            cache = pickle.load(f)\n",
    "    else:\n",
    "        cache = {}\n",
    "\n",
    "    embeddings = []\n",
    "    new_count = 0\n",
    "\n",
    "    for text in tqdm(texts, desc=\"Embedding\"):\n",
    "        key = hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "        if key in cache:\n",
    "            embeddings.append(cache[key])\n",
    "        else:\n",
    "            try:\n",
    "                response = openai.embeddings.create(input=text, model=model)\n",
    "                vec = response.data[0].embedding\n",
    "                cache[key] = vec\n",
    "                embeddings.append(vec)\n",
    "                new_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"OpenAI API failed on: {text[:50]}...\\n{e}\")\n",
    "                vec = [0.0] * 1536  # fallback, dim of text-embedding-3-small\n",
    "                embeddings.append(vec)\n",
    "\n",
    "    # Save updated cache\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "    print(f\"Embedding complete. {new_count} new vectors generated, {len(texts) - new_count} loaded from cache.\")\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272fa567-4767-468c-bee0-54101350651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tb1_path, tb2_path):\n",
    "    table1 = pd.read_csv(tb1_path)\n",
    "    table2 = pd.read_csv(tb2_path)\n",
    "    return table1, table2\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_table(df, columns_to_concat):\n",
    "    # Fill NaN for each selected column\n",
    "    for col in columns_to_concat:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in dataframe.\")\n",
    "        df[col] = df[col].fillna('')\n",
    "\n",
    "    # Apply preprocess and concatenate\n",
    "    processed_text = ''\n",
    "    for col in columns_to_concat:\n",
    "        processed_text += df[col].apply(preprocess) + ' '\n",
    "    \n",
    "    df['processed'] = processed_text.str.strip()\n",
    "    df['processed'] = df['processed'].replace('', 'unknown')\n",
    "    return df\n",
    "\n",
    "def preprocess_tables(table1, table2, table1_columns, table2_columns):\n",
    "    table1 = preprocess_table(table1, table1_columns)\n",
    "    table2 = preprocess_table(table2, table2_columns)\n",
    "    return table1, table2\n",
    "\n",
    "def encode_texts(model, texts, batch_size=64):\n",
    "    print(\"\\nEncoding texts into embeddings...\")\n",
    "    start_time = time.time()\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, batch_size=batch_size)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Embedding generation completed in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    if np.isnan(embeddings).any():\n",
    "        print(\"Warning: NaN values detected in embeddings - replacing with zeros\")\n",
    "        embeddings = np.nan_to_num(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1)\n",
    "    norms[norms == 0] = 1e-10\n",
    "    return embeddings / norms[:, np.newaxis]\n",
    "\n",
    "def build_faiss_index(embeddings, batch_size=1000):\n",
    "    print(\"\\nBuilding FAISS index...\")\n",
    "    start_time = time.time()\n",
    "    dimension = embeddings.shape[1]\n",
    "    if dimension == 0:\n",
    "        raise ValueError(\"Embedding dimension is 0 - check your input data\")\n",
    "    \n",
    "    try:\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        for i in tqdm(range(0, len(embeddings), batch_size), desc=\"Adding batches to FAISS\"):\n",
    "            batch = embeddings[i:i+batch_size]\n",
    "            if not np.isnan(batch).any():\n",
    "                index.add(batch)\n",
    "            else:\n",
    "                print(f\"Skipping batch {i} due to NaN values\")\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"FAISS index built in {elapsed:.2f} seconds.\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating FAISS index: {e}\")\n",
    "        print(\"Using brute-force matching instead\")\n",
    "        return None\n",
    "\n",
    "def find_matches(query_embeddings, table1, target_df, target_embeddings, index=None, top_k=30):\n",
    "    if index is None:\n",
    "        raise ValueError(\"FAISS index is not available. Matching cannot proceed.\")\n",
    "    print(\"\\nFinding matches...\")\n",
    "    matches = []\n",
    "    start_time = time.time()\n",
    "    for i, query_embedding in enumerate(tqdm(query_embeddings, desc=\"Matching queries\")):\n",
    "        if np.isnan(query_embedding).any():\n",
    "            print(f\"Skipping query {i} due to NaN values\")\n",
    "            continue\n",
    "\n",
    "        if index:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "            distances, indices = index.search(query_embedding, top_k)\n",
    "            distances = distances[0]\n",
    "            indices = indices[0]\n",
    "        # else:\n",
    "        #     similarities = np.dot(target_embeddings, query_embedding)\n",
    "        #     indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        #     distances = similarities[indices]\n",
    "\n",
    "        for score, idx in zip(distances, indices):\n",
    "            matches.append({\n",
    "                'left_id': table1.iloc[i]['ID'],\n",
    "                'right_id': target_df.iloc[idx]['ID'],\n",
    "                'similarity_score': score,\n",
    "                'table1_text': table1.iloc[i]['processed'],\n",
    "                'table2_text': target_df.iloc[idx]['processed']\n",
    "            })\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Matching completed in {elapsed:.2f} seconds.\")\n",
    "    return matches\n",
    "\n",
    "def save_results(matches_df, index=None, matches_file=\"entity_matches.csv\", index_file=\"entity_matching_index.faiss\"):\n",
    "    matches_df.to_csv(matches_file, index=False)\n",
    "    print(f\"\\nSaved matches to {matches_file}.\")\n",
    "    if index:\n",
    "        faiss.write_index(index, index_file)\n",
    "        print(f\"Saved FAISS index to {index_file}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2713d6f5-759e-4306-a292-9cc52f51417a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding texts using OpenAI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|███████████████████████████████████████████████| 4259/4259 [27:26<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding complete. 4217 new vectors generated, 42 loaded from cache.\n",
      "\n",
      "Encoding texts using OpenAI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|███████████████████████████████████████████████| 5001/5001 [33:00<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding complete. 4639 new vectors generated, 362 loaded from cache.\n",
      "\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches to FAISS: 100%|██████████████████████████████████████| 6/6 [00:00<00:00, 143.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built in 0.05 seconds.\n",
      "\n",
      "Finding matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|███████████████████████████████████████| 4259/4259 [00:13<00:00, 324.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed in 13.12 seconds.\n",
      "\n",
      "Top matches between tables:\n",
      "       left_id  right_id  similarity_score  \\\n",
      "11490      384       449          0.984726   \n",
      "59610     1988      3563          0.981807   \n",
      "30180     1007       294          0.963221   \n",
      "28530      952      3225          0.961034   \n",
      "67950     2266      2419          0.956337   \n",
      "\n",
      "                                             table1_text  \\\n",
      "11490  lenovo lenovo - g50 156 laptop - intel core i3...   \n",
      "59610  other 3m privacy filter for widescreen laptop ...   \n",
      "30180  dell dell - inspiron 156 4k ultra hd touch-scr...   \n",
      "28530  other cooler master notepal x3 - gaming laptop...   \n",
      "67950  other solo pro 173 laptop backpack black pro742-4   \n",
      "\n",
      "                                             table2_text  \n",
      "11490  lenovo lenovo g50 156 laptop intel core i3 4gb...  \n",
      "59610  other 3m privacy filter for widescreen laptop ...  \n",
      "30180  dell dell inspiron 156 4k ultra hd touchscreen...  \n",
      "28530  other cooler master notepal x3 gaming laptop c...  \n",
      "67950      other solo pro laptop backpack black pro742-4  \n",
      "\n",
      "Saved matches to entity_matches.csv.\n",
      "Saved FAISS index to entity_matching_index.faiss.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # File paths\n",
    "    tb1_path = os.path.join(\"files\", \"amazon.csv\")\n",
    "    tb2_path = os.path.join(\"files\", \"best_buy.csv\")\n",
    "\n",
    "    # Load and preprocess data\n",
    "    table1, table2 = load_data(tb1_path, tb2_path)\n",
    "    \n",
    "    table1_columns = [\"Brand\", \"Name\"]        \n",
    "    table2_columns = [\"Brand\", \"Name\"]     \n",
    "\n",
    "    # table1_columns = [\"Brand\", \"Name\", \"Features\"]        \n",
    "    # table2_columns = [\"Brand\", \"Name\", \"Description\"]     \n",
    "    \n",
    "    table1, table2 = preprocess_tables(table1, table2, table1_columns, table2_columns)\n",
    "\n",
    "    # Load model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    # model = SentenceTransformer(\"all-mpnet-base-v2\") model is too big\n",
    "\n",
    "    # Generate and normalize embeddings\n",
    "    # embeddings1 = encode_texts(model, table1['processed'].tolist())\n",
    "    # embeddings2 = encode_texts(model, table2['processed'].tolist())\n",
    "    # embeddings1 = normalize_embeddings(embeddings1)\n",
    "    # embeddings2 = normalize_embeddings(embeddings2)\n",
    "\n",
    "\n",
    "    # use the openai\n",
    "    # embeddings1 = encode_texts_openai(table1['processed'].tolist(), cache_path=\"table1_embeddings.pkl\")\n",
    "    # embeddings2 = encode_texts_openai(table2['processed'].tolist(), cache_path=\"table2_embeddings.pkl\")\n",
    "    \n",
    "    # embeddings1 = normalize_embeddings(embeddings1)\n",
    "    # embeddings2 = normalize_embeddings(embeddings2)\n",
    "\n",
    "    # Build FAISS index\n",
    "    index = build_faiss_index(embeddings2)\n",
    "\n",
    "    # Find matches\n",
    "    matches = find_matches(embeddings1, table1, table2, embeddings2, index=index, top_k=30)\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "\n",
    "    # Display some results\n",
    "    print(\"\\nTop matches between tables:\")\n",
    "    print(matches_df.sort_values(by='similarity_score', ascending=False).head())\n",
    "\n",
    "    # Save results\n",
    "    save_results(matches_df, index)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe212d-bf04-4a93-ac98-feff924474da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
